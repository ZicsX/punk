{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"text-align: center\">\n",
    "    <img src=\"https://y.yarn.co/879fb637-70a2-4697-9dbf-e078573403e6_text.gif\" alt=\"Alt Text\" style=\"display: block; margin: 0 auto;\">\n",
    "</div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import AutoTokenizer\n",
    "from huggingface_hub import hf_hub_download\n",
    "from transformers import BertModel\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "# Download pytorch model\n",
    "\n",
    "# Define the repository name and filename\n",
    "repo_name = \"zicsx/Hindi-Punk\"\n",
    "filename = \"Hindi-Punk-model.pth\"\n",
    "\n",
    "# Download the file\n",
    "model_path = hf_hub_download(repo_id=repo_name, filename=filename)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Punctuation Model Class\n",
    "\n",
    "class CustomTokenClassifier(nn.Module):\n",
    "    def __init__(self, hidden_size, num_classes):\n",
    "        super(CustomTokenClassifier, self).__init__()\n",
    "        if num_classes > 0:\n",
    "            self.classifier = nn.Linear(hidden_size, num_classes)\n",
    "        else:\n",
    "            self.classifier = None\n",
    "\n",
    "    def forward(self, hidden_states):\n",
    "        if self.classifier:\n",
    "            return self.classifier(hidden_states)\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "class PunctuationModel(nn.Module):\n",
    "    def __init__(self, bert_model_name, punct_num_classes, hidden_size):\n",
    "        super(PunctuationModel, self).__init__()\n",
    "        self.bert = BertModel.from_pretrained(bert_model_name)\n",
    "        self.punct_classifier = CustomTokenClassifier(hidden_size, punct_num_classes)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask, token_type_ids):\n",
    "        hidden_states = self.bert(input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)[0]\n",
    "        punct_logits = self.punct_classifier(hidden_states) if self.punct_classifier else None\n",
    "        return punct_logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = PunctuationModel(\n",
    "    bert_model_name='google/muril-base-cased',\n",
    "    punct_num_classes=5,  # Number of punctuation classes (including 'O')\n",
    "    hidden_size=768       # Hidden size of the BERT model\n",
    ")\n",
    "\n",
    "model.load_state_dict(torch.load(model_path))\n",
    "# model.eval()  # Set the model to evaluation mode\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[  104,  1840,  6345,  1145,  4254, 48690, 13570, 20597,  2044,   105]])\n"
     ]
    }
   ],
   "source": [
    "# Load and test the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "                    pretrained_model_name_or_path=\"zicsx/Hindi-Punk\", use_fast=True,\n",
    "                )\n",
    "# test an example input\n",
    "example_text = \"‡§Ü‡§™ ‡§ï‡•à‡§∏‡•á ‡§π‡•à‡§Ç ‡§Æ‡•Å‡§ù‡•á ‡§Ü‡§™‡§∏‡•á ‡§Æ‡§ø‡§≤‡§ï‡§∞ ‡§ñ‡•Å‡§∂‡•Ä ‡§π‡•Å‡§à\"\n",
    "encoded_input = tokenizer(example_text, return_tensors=\"pt\")\n",
    "print(encoded_input['input_ids'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to perform inference and get punctuation and capitalization predictions\n",
    "def predict_punctuation_capitalization(model, text, tokenizer):\n",
    "    # Tokenize the input text\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "\n",
    "    # Determine the device to use (CPU or GPU)\n",
    "    device = next(model.parameters()).device\n",
    "\n",
    "    # Move inputs to the same device as the model\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "\n",
    "    # Perform inference\n",
    "    with torch.no_grad():\n",
    "        punct_logits = model(**inputs)\n",
    "\n",
    "    # Convert logits to probabilities and get the indices of the highest probability labels\n",
    "    punct_probs = torch.nn.functional.softmax(punct_logits, dim=-1)\n",
    "    punct_predictions = torch.argmax(punct_probs, dim=-1)\n",
    "\n",
    "    return punct_predictions\n",
    "\n",
    "# Function to map predictions to labels and combine them with the original text\n",
    "def combine_predictions_with_text(text, tokenizer, punct_predictions, punct_index_to_label):\n",
    "    # Tokenize the input text and get offset mappings\n",
    "    encoded = tokenizer.encode_plus(text, return_tensors='pt', return_offsets_mapping=True)\n",
    "    tokens = tokenizer.convert_ids_to_tokens(encoded['input_ids'][0])\n",
    "    offset_mapping = encoded['offset_mapping'][0].tolist()\n",
    "\n",
    "    # Combine tokens with their predictions\n",
    "    combined = []\n",
    "    current_word = ''\n",
    "    current_punct = ''\n",
    "    for i, (token, punct) in enumerate(zip(tokens, punct_predictions.squeeze())):\n",
    "        # Skip special tokens\n",
    "        if token in [tokenizer.cls_token, tokenizer.sep_token, tokenizer.pad_token]:\n",
    "            continue\n",
    "\n",
    "        # Remove \"##\" prefix from subword tokens\n",
    "        if token.startswith(\"##\"):\n",
    "            token = token[2:]\n",
    "        else:\n",
    "            # If not the first token, add a space before starting a new word\n",
    "            if current_word:\n",
    "                combined.append(current_word + current_punct)\n",
    "                current_word = ''\n",
    "                current_punct = ''\n",
    "        \n",
    "        current_word += token\n",
    "\n",
    "        # Update the current punctuation if predicted\n",
    "        if punct_index_to_label[punct.item()] != 'O':\n",
    "            current_punct = punct_index_to_label[punct.item()]\n",
    "\n",
    "    # Append the last word and punctuation (if any) to the combined text\n",
    "    combined.append(current_word + current_punct)\n",
    "\n",
    "    return ' '.join(combined)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined Text: ‡§∏‡§≤‡§æ‡§Æ‡§Ö‡§≤‡•à‡§ï‡•Å‡§Æ, ‡§ï‡§π‡§æ‡§Å ‡§ú‡§æ ‡§∞‡§π‡•á ‡§π‡•à‡§Ç ‡§ú‡•Ä? ‡§Ü‡§ì, ‡§¨‡•à‡§†‡•ã, ‡§õ‡•ã‡§°‡§º ‡§¶‡•á‡§§‡§æ ‡§π‡•Ç‡§Å? ‡§π‡•á‡§≤‡•ã, ‡§è‡§ï‡•ç‡§∏‡§ï‡•ç‡§Ø‡•Ç‡§ú, ‡§Æ‡•Ä ‡§Ü‡§™‡§ï‡§æ ‡§ï‡•ç‡§Ø‡§æ ‡§®‡§æ‡§Æ ‡§π‡•à? ‡§§‡•Å‡§Æ ‡§≤‡•ã‡§ó‡•ã‡§Ç ‡§ï‡•ã ‡§¨‡§æ‡§¶ ‡§Æ‡•á‡§Ç ‡§¶‡•á‡§ñ ‡§≤‡•á‡§§‡§æ ‡§π‡•Ç‡§Å‡•§\n"
     ]
    }
   ],
   "source": [
    "# Punctuation label to index mapping\n",
    "punct_index_to_label = {0: '', 1: '!', 2: ',', 3: '?', 4: '‡•§'}\n",
    "\n",
    "# Example usage\n",
    "text = \"‡§∏‡§≤‡§æ‡§Æ‡§Ö‡§≤‡•à‡§ï‡•Å‡§Æ ‡§ï‡§π‡§æ‡§Å ‡§ú‡§æ ‡§∞‡§π‡•á ‡§π‡•à‡§Ç ‡§ú‡•Ä ‡§Ü‡§ì ‡§¨‡•à‡§†‡•ã ‡§õ‡•ã‡§°‡§º ‡§¶‡•á‡§§‡§æ ‡§π‡•Ç‡§Å ‡§π‡•á‡§≤‡•ã ‡§è‡§ï‡•ç‡§∏‡§ï‡•ç‡§Ø‡•Ç‡§ú ‡§Æ‡•Ä ‡§Ü‡§™‡§ï‡§æ ‡§ï‡•ç‡§Ø‡§æ ‡§®‡§æ‡§Æ ‡§π‡•à ‡§§‡•Å‡§Æ ‡§≤‡•ã‡§ó‡•ã‡§Ç ‡§ï‡•ã ‡§¨‡§æ‡§¶ ‡§Æ‡•á‡§Ç ‡§¶‡•á‡§ñ ‡§≤‡•á‡§§‡§æ ‡§π‡•Ç‡§Å\"\n",
    "\n",
    "# Predict punctuation\n",
    "punct_predictions = predict_punctuation_capitalization(model, text, tokenizer)\n",
    "\n",
    "# Combine predictions with the original text\n",
    "combined_text = combine_predictions_with_text(text, tokenizer, punct_predictions, punct_index_to_label)\n",
    "print(\"Combined Text:\", combined_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First lets see how the tokenizer is working and how we can implement pre-processing and post-processing steps to fix the issues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: ‡§®‡§Æ‡§∏‡•ç‡§§‡•á ‡§¶‡•Å‡§®‡§ø‡§Ø‡§æ‡•§\n",
      "‡§ï‡•à‡§∏‡•á ‡§π‡•ã?\n",
      "Decoded: [CLS] ‡§®‡§Æ‡§∏‡•ç‡§§‡•á ‡§¶‡•Å‡§®‡§ø‡§Ø‡§æ ‡•§ ‡§ï‡•à‡§∏‡•á ‡§π‡•ã? [SEP]\n",
      "\n",
      "\n",
      "Original: ‡§Ø‡§π ‡§è‡§ï ‡§™‡§∞‡•Ä‡§ï‡•ç‡§∑‡§£ ‡§π‡•à‡•§\n",
      "#‡§µ‡§ø‡§∂‡•á‡§∑_‡§µ‡§∞‡•ç‡§£\n",
      "Decoded: [CLS] ‡§Ø‡§π ‡§è‡§ï ‡§™‡§∞‡•Ä‡§ï‡•ç‡§∑‡§£ ‡§π‡•à ‡•§ # ‡§µ‡§ø‡§∂‡•á‡§∑ _ ‡§µ‡§∞‡•ç‡§£ [SEP]\n",
      "\n",
      "\n",
      "Original: ‡§™‡•ç‡§∞‡•ã‡§ó‡•ç‡§∞‡§æ‡§Æ‡§ø‡§Ç‡§ó ‡§Æ‡•á‡§Ç ‡§®‡§Ø‡§æ ‡§™‡§Ç‡§ï‡•ç‡§§‡§ø ‡§µ‡§ø‡§≠‡§æ‡§ú‡§ï‡•§\n",
      "‡§®‡§à ‡§≤‡§æ‡§á‡§®‡•§\n",
      "‡§î‡§∞ ‡§è‡§ï‡•§\n",
      "Decoded: [CLS] ‡§™‡•ç‡§∞‡•ã‡§ó‡•ç‡§∞‡§æ‡§Æ‡§ø‡§Ç‡§ó ‡§Æ‡•á‡§Ç ‡§®‡§Ø‡§æ ‡§™‡§Ç‡§ï‡•ç‡§§‡§ø ‡§µ‡§ø‡§≠‡§æ‡§ú‡§ï ‡•§ ‡§®‡§à ‡§≤‡§æ‡§á‡§® ‡•§ ‡§î‡§∞ ‡§è‡§ï ‡•§ [SEP]\n",
      "\n",
      "\n",
      "Original: ‡§™‡§π‡§≤‡•Ä ‡§™‡§Ç‡§ï‡•ç‡§§‡§ø‡•§\n",
      "‡§¶‡•Ç‡§∏‡§∞‡•Ä ‡§™‡§Ç‡§ï‡•ç‡§§‡§ø‡•§\n",
      "‡§§‡•Ä‡§∏‡§∞‡•Ä ‡§™‡§Ç‡§ï‡•ç‡§§‡§ø‡•§\n",
      "Decoded: [CLS] ‡§™‡§π‡§≤‡•Ä ‡§™‡§Ç‡§ï‡•ç‡§§‡§ø ‡•§ ‡§¶‡•Ç‡§∏‡§∞‡•Ä ‡§™‡§Ç‡§ï‡•ç‡§§‡§ø ‡•§ ‡§§‡•Ä‡§∏‡§∞‡•Ä ‡§™‡§Ç‡§ï‡•ç‡§§‡§ø ‡•§ [SEP]\n",
      "\n",
      "\n",
      "Original: ‡§µ‡§ø‡§∞‡§æ‡§Æ ‡§ö‡§ø‡§π‡•ç‡§®: , ; : ' \" ( ) [ ] { }\n",
      "Decoded: [CLS] ‡§µ‡§ø‡§∞‡§æ‡§Æ ‡§ö‡§ø‡§π‡•ç‡§® :, ; :'\" ( ) [ ] { } [SEP]\n",
      "\n",
      "\n",
      "Original: Hello ‡§®‡§Æ‡§∏‡•ç‡§§‡•á 123 ‡•ß‡•®‡•© + - = * /\n",
      "Decoded: [CLS] Hello ‡§®‡§Æ‡§∏‡•ç‡§§‡•á 123 ‡•ß‡•®‡•© + - = * / [SEP]\n",
      "\n",
      "\n",
      "Original: ‡§Ø‡§π ‡§è‡§ï ‡§∏‡•ç‡§Æ‡§æ‡§á‡§≤‡•Ä ‡§π‡•à :) üòâ\n",
      "Decoded: [CLS] ‡§Ø‡§π ‡§è‡§ï ‡§∏‡•ç‡§Æ‡§æ‡§á‡§≤‡•Ä ‡§π‡•à : ) [UNK] [SEP]\n",
      "\n",
      "\n",
      "Original: ‡§Ø‡§π ‡§è‡§ï ‡§à‡§Æ‡•ã‡§ú‡•Ä ‡§π‡•à üåüüöÄ\n",
      "Decoded: [CLS] ‡§Ø‡§π ‡§è‡§ï ‡§à‡§Æ‡•ã‡§ú‡•Ä ‡§π‡•à [UNK] [SEP]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "                    pretrained_model_name_or_path=\"zicsx/Hindi-Punk\", use_fast=True,\n",
    "                )\n",
    "# Test cases with Hindi text strings, newline separators, and special characters\n",
    "test_cases = [\n",
    "    \"‡§®‡§Æ‡§∏‡•ç‡§§‡•á ‡§¶‡•Å‡§®‡§ø‡§Ø‡§æ‡•§\\n‡§ï‡•à‡§∏‡•á ‡§π‡•ã?\",\n",
    "    \"‡§Ø‡§π ‡§è‡§ï ‡§™‡§∞‡•Ä‡§ï‡•ç‡§∑‡§£ ‡§π‡•à‡•§\\n#‡§µ‡§ø‡§∂‡•á‡§∑_‡§µ‡§∞‡•ç‡§£\",\n",
    "    \"‡§™‡•ç‡§∞‡•ã‡§ó‡•ç‡§∞‡§æ‡§Æ‡§ø‡§Ç‡§ó ‡§Æ‡•á‡§Ç ‡§®‡§Ø‡§æ ‡§™‡§Ç‡§ï‡•ç‡§§‡§ø ‡§µ‡§ø‡§≠‡§æ‡§ú‡§ï‡•§\\n‡§®‡§à ‡§≤‡§æ‡§á‡§®‡•§\\n‡§î‡§∞ ‡§è‡§ï‡•§\",\n",
    "    \"‡§™‡§π‡§≤‡•Ä ‡§™‡§Ç‡§ï‡•ç‡§§‡§ø‡•§\\n‡§¶‡•Ç‡§∏‡§∞‡•Ä ‡§™‡§Ç‡§ï‡•ç‡§§‡§ø‡•§\\n‡§§‡•Ä‡§∏‡§∞‡•Ä ‡§™‡§Ç‡§ï‡•ç‡§§‡§ø‡•§\",\n",
    "    \"‡§µ‡§ø‡§∞‡§æ‡§Æ ‡§ö‡§ø‡§π‡•ç‡§®: , ; : ' \\\" ( ) [ ] { }\",\n",
    "    \"Hello ‡§®‡§Æ‡§∏‡•ç‡§§‡•á 123 ‡•ß‡•®‡•© + - = * /\",\n",
    "    \"‡§Ø‡§π ‡§è‡§ï ‡§∏‡•ç‡§Æ‡§æ‡§á‡§≤‡•Ä ‡§π‡•à :) üòâ\",\n",
    "    \"‡§Ø‡§π ‡§è‡§ï ‡§à‡§Æ‡•ã‡§ú‡•Ä ‡§π‡•à üåüüöÄ\"\n",
    "]\n",
    "\n",
    "# Tokenize and decode each test case\n",
    "for test_case in test_cases:\n",
    "    encoded = tokenizer.encode(test_case)\n",
    "    decoded = tokenizer.decode(encoded)\n",
    "    print(f\"Original: {test_case}\")\n",
    "    print(f\"Decoded: {decoded}\")\n",
    "    print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the outputs from the test cases, we can conclude the following about its handling of text:\n",
    "\n",
    "1. **New Line Characters**: The tokenizer does not preserve new line characters (`\\n`). Text that was originally separated by new lines is merged into a single line in the decoded output.\n",
    "\n",
    "2. **Punctuation and Special Characters**: The tokenizer tends to separate certain punctuation marks from the words they follow, such as the Hindi full stop (‡•§). However, other punctuation marks like commas, semicolons, and brackets are preserved as in the original text. Special characters like hashtags (`#`) and underscores (`_`) are also separated from the surrounding text.\n",
    "\n",
    "3. **Mixed English and Hindi Text**: The tokenizer effectively handles text containing a mix of English and Hindi characters, as well as numbers. The decoded text remains faithful to the original in this aspect.\n",
    "\n",
    "4. **Emoticons and Emoji**: The tokenizer does not handle emoticons and emoji well. In the test cases, the emoticon `:)` is split into `:` and `)`, and the emoji `üåüüöÄ` is replaced by `[UNK]` (unknown token), indicating the tokenizer may not have representations for certain emoticons and emoji.\n",
    "\n",
    "5. **Special Tokens**: The tokenizer adds special tokens `[CLS]` at the beginning and `[SEP]` at the end of each decoded sequence. These tokens are used in transformer models for classification (`[CLS]`) and separation (`[SEP]`) tasks.\n",
    "\n",
    "6. **Consistency in Decoding**: Aside from the above points, the tokenizer consistently decodes the text in a way that is mostly faithful to the original content, particularly with respect to the words and their order.\n",
    "\n",
    "These findings highlight the tokenizer's behavior in handling different types of input, including its treatment of new line characters, punctuation, mixed language content, and special characters. Understanding these behaviors is important for effectively using the tokenizer in natural language processing tasks, especially those involving Hindi text.\n",
    "\n",
    "---\n",
    "\n",
    "To address the issues identified with the tokenizer, we can implement pre-processing and post-processing steps. Here are some suggestions:\n",
    "\n",
    "### Pre-processing:\n",
    "1. **Replace New Line Characters**: Replace new line characters (`\\n`) with a special token or a unique string that we can easily identify and convert back to new lines in post-processing.\n",
    "\n",
    "2. **Handle Special Characters and Punctuation**: If certain punctuation marks are being incorrectly separated, consider replacing them with equivalent tokens or merging them with adjacent words before tokenization.\n",
    "\n",
    "3. **Encode Emoticons and Emoji**: Convert emoticons and emoji into text representations or special tokens that the tokenizer can handle.\n",
    "\n",
    "### Post-processing:\n",
    "1. **Restore New Line Characters**: Convert the special tokens or unique strings used to represent new lines back into actual new line characters (`\\n`).\n",
    "\n",
    "2. **Adjust Punctuation and Special Characters**: If punctuation or special characters were modified during pre-processing, revert them to their original form.\n",
    "\n",
    "3. **Handle Special Tokens (`[CLS]` and `[SEP]`)**: Remove the `[CLS]` and `[SEP]` tokens added by the tokenizer, if they are not needed for your specific task.\n",
    "\n",
    "4. **Decode Emoticons and Emoji**: If emoticons and emoji were converted to text representations or special tokens, convert them back to their original form.\n",
    "\n",
    "By applying these pre-processing and post-processing steps, we can mitigate some of the issues observed with the tokenizer, ensuring that the input text is handled more accurately and the output aligns better with your requirements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
